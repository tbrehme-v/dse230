{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf62860-b939-47e1-b7c0-1c4fa2c712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade dask[complete] matplotlib seaborn pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7629ec40-00cf-42cb-b8a6-11be4e6687d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed, compute\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1098e684-c30a-42b7-a5e2-9868afcfe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "path = \"/home/work/Final/\"\n",
    "dataset_files = [\n",
    "    \"features_watch_a.csv\",\n",
    "    \"features_watch_g.csv\",\n",
    "    \"features_phone_a.csv\",\n",
    "    \"features_phone_g.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb05e02-d5d1-44ad-8956-172ed5d3f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_classification_reports(reports):\n",
    "    keys = list(reports[0].keys())\n",
    "    avg_report = {}\n",
    "    all_class_labels = set()\n",
    "    for r in reports:\n",
    "        all_class_labels.update([k for k in r.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']])\n",
    "    all_class_labels = sorted(all_class_labels, key=lambda x: (not x.isdigit(), x))\n",
    "    for k in keys:\n",
    "        if k == 'accuracy':\n",
    "            avg_report[k] = np.mean([r.get(k, 0) for r in reports])\n",
    "        elif isinstance(reports[0][k], dict):\n",
    "            avg_report[k] = {}\n",
    "            if k in all_class_labels or k in ['macro avg', 'weighted avg']:\n",
    "                metrics = set()\n",
    "                for r in reports:\n",
    "                    if k in r:\n",
    "                        metrics.update(r[k].keys())\n",
    "                for metric in metrics:\n",
    "                    vals = [r[k][metric] for r in reports if k in r and metric in r[k]]\n",
    "                    avg_report[k][metric] = np.mean(vals) if vals else 0\n",
    "        else:\n",
    "            avg_report[k] = np.mean([r.get(k, 0) for r in reports])\n",
    "    for class_label in all_class_labels:\n",
    "        if class_label not in avg_report:\n",
    "            avg_report[class_label] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0}\n",
    "    return avg_report\n",
    "\n",
    "summary_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e2b09-2124-4a47-aeb4-007e0e920eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in dataset_files:\n",
    "    print(f\"\\n=== Running LOSO on {file} ===\")\n",
    "    df_dd = dd.read_csv(path + file)\n",
    "    df = df_dd.compute()\n",
    "\n",
    "    subject_ids = df['sub_id'].unique()\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    df['activity_encoded'] = label_enc.fit_transform(df['activity'])\n",
    "\n",
    "    drop_cols = ['sub_id', 'group_id', 'activity', 'activity_encoded']\n",
    "    X_all = df.drop(columns=drop_cols)\n",
    "    y_all = df['activity_encoded']\n",
    "\n",
    "    n_classes = len(label_enc.classes_)\n",
    "    n_features = X_all.shape[1]\n",
    "\n",
    "    @delayed\n",
    "    def run_loso_fold(test_sub):\n",
    "        train_mask = df['sub_id'] != test_sub\n",
    "        test_mask = df['sub_id'] == test_sub\n",
    "\n",
    "        X_train = X_all.loc[train_mask]\n",
    "        y_train = y_all.loc[train_mask]\n",
    "        X_test = X_all.loc[test_mask]\n",
    "        y_test = y_all.loc[test_mask]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=5, min_samples_split=5, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_train_pred = rf.predict(X_train_scaled)\n",
    "        y_test_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_test_pred, labels=range(n_classes))\n",
    "        report = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)\n",
    "        feature_importances = rf.feature_importances_\n",
    "\n",
    "        return train_acc, test_acc, cm, report, feature_importances\n",
    "\n",
    "    results = [run_loso_fold(test_sub) for test_sub in subject_ids]\n",
    "    train_accs, test_accs, cms, reports, feature_imps = zip(*compute(*results))\n",
    "\n",
    "    avg_train_acc = np.mean(train_accs)\n",
    "    avg_test_acc = np.mean(test_accs)\n",
    "    conf_matrix_sum = np.sum(cms, axis=0)\n",
    "    avg_feature_importances = np.mean(feature_imps, axis=0)\n",
    "    conf_matrix_avg = conf_matrix_sum / conf_matrix_sum.sum(axis=1, keepdims=True)\n",
    "\n",
    "    summary_results.append({\n",
    "        \"Dataset\": file,\n",
    "        \"Train Accuracy\": avg_train_acc,\n",
    "        \"Test Accuracy\": avg_test_acc\n",
    "    })\n",
    "    \n",
    "    avg_report = average_classification_reports(reports)\n",
    "\n",
    "    print(f\"Average Train Accuracy: {avg_train_acc:.3f}\")\n",
    "    print(f\"Average Test Accuracy:  {avg_test_acc:.3f}\")\n",
    "    print(\"\\nAverage Confusion Matrix (Normalized by True Class):\")\n",
    "    print(conf_matrix_avg)\n",
    "    print(\"\\nAverage Confusion Matrix (Raw counts):\")\n",
    "    print(conf_matrix_sum)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix_sum, annot=True, fmt='d',\n",
    "                xticklabels=label_enc.classes_, yticklabels=label_enc.classes_)\n",
    "    plt.title(f\"Confusion Matrix (Raw Counts) - {file}\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix_avg, annot=True, fmt='.2f',\n",
    "                xticklabels=label_enc.classes_, yticklabels=label_enc.classes_)\n",
    "    plt.title(f\"Confusion Matrix (Normalized by True Class) - {file}\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAverage Classification Report:\")\n",
    "    for label_idx, label_name in enumerate(label_enc.classes_):\n",
    "        pr = avg_report[str(label_idx)]\n",
    "        print(f\"{label_name}: Precision: {pr['precision']:.3f}, Recall: {pr['recall']:.3f}, F1-score: {pr['f1-score']:.3f}\")\n",
    "\n",
    "    print(f\"\\nOverall Accuracy: {avg_report['accuracy']:.3f}\")\n",
    "\n",
    "    feature_names = X_all.columns\n",
    "    indices = np.argsort(avg_feature_importances)[::-1]\n",
    "    top_n = 45\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"Average Feature Importances (Random Forest LOSO) - {file}\")\n",
    "    plt.bar(range(top_n), avg_feature_importances[indices[:top_n]], align='center')\n",
    "    plt.xticks(range(top_n), feature_names[indices[:top_n]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470c8bf9-2063-41d3-b74c-317774bb69f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary of Train and Test Accuracies ===\n",
      "             Dataset  Train Accuracy  Test Accuracy\n",
      "features_watch_a.csv        0.624856       0.625979\n",
      "features_watch_g.csv        0.591107       0.557044\n",
      "features_phone_a.csv        0.408191       0.314701\n",
      "features_phone_g.csv        0.386601       0.327061\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"\\n=== Summary of Train and Test Accuracies ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
