{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e2cbd4-a9c6-4869-a9b6-5aafa996a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0d3f-9ef5-436d-a0ba-8a1b04e42c2e",
   "metadata": {},
   "source": [
    "## Initilization and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd3ed2b-1b59-4f01-b233-92fa49558ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# need these for the feature generation and UDFs and functions.col for brevity \n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StructType, StructField\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a577b09d-0b17-474e-ad8e-10058b3e3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"EDA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cff669-8b11-4c54-903b-e7b7b1f8cb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "###---------- Change path if necessary ----------###\n",
    "path = \"file:///home/work/Final/Dataset/raw\"\n",
    "\n",
    "file = \"/phone/accel/*.txt\"\n",
    "df_phone_a = spark.read.csv(path + file, header=False, inferSchema=True)\n",
    "\n",
    "file = \"/phone/gyro/*.txt\"\n",
    "df_phone_g = spark.read.csv(path + file, header=False, inferSchema=True)\n",
    "\n",
    "file = \"/watch/accel/*.txt\"\n",
    "df_watch_a = spark.read.csv(path + file, header=False, inferSchema=True)\n",
    "\n",
    "file = \"/watch/gyro/*.txt\"\n",
    "df_watch_g = spark.read.csv(path + file, header=False, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5497d7bb-ae25-496a-8801-4d89d0a34784",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 20 #Hz    # also defined in a UDF!\n",
    "SECONDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3163f1d9-e454-43a3-800a-14609929433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Phone Accel: {df_phone_a.count()}\")\n",
    "# print(f\"Phone Gyro: {df_phone_g.count()}\")\n",
    "# print(f\"Watch Accel: {df_watch_a.count()}\")\n",
    "# print(f\"Watch Gyro: {df_watch_g.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82143e-2a02-4c16-aa32-9c1843872168",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e24ffe-7d0d-4aec-a392-30d15915d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_df(df): \n",
    "    #Rename Cols\n",
    "    col_map = {'_c0': 'sub_id',\n",
    "           '_c1': 'activity',\n",
    "           '_c2': 'time',\n",
    "           '_c3': 'x',\n",
    "           '_c4': 'y',\n",
    "           '_c5': 'z'\n",
    "          }\n",
    "    df = df.withColumnsRenamed(col_map)\n",
    "\n",
    "    #convert z from str to double \n",
    "    df = df.withColumn('z', func.regexp_replace(\"z\", \";\", \"\").cast(\"double\"))\n",
    "\n",
    "    #Sort\n",
    "    df = df.sort(\"sub_id\", \"activity\", \"time\")\n",
    "\n",
    "    #Drop any na/null \n",
    "    count1 = df.count()\n",
    "    df = df.dropna()\n",
    "    print(f\"rows_dropped: {count1 - df.count()}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d930-17cd-4e27-86da-3dd29ec9b588",
   "metadata": {},
   "source": [
    "## Segmenting\n",
    "\n",
    "Segmenting the samples in to ~10 second segments will let us do some more feature enginering \n",
    "\n",
    "3 mins / 10s = 18 segment per activity per subject \n",
    "\n",
    "51 subjects * 18 segment = 918 per activity \n",
    "\n",
    "16,524 segments across all activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d198237c-381b-4a71-9de1-2e8ae835fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment into 10s of data (200 samples) \n",
    "\n",
    "def label_segments(df, num_s):\n",
    "    \"\"\" takes a spark df and num_s (int) returns a df where each row element is a list num_s elements\n",
    "\n",
    "    df must have cols as named in clean_df()\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(\"sub_id\", \"activity\").orderBy(\"time\")\n",
    "\n",
    "    df = df.withColumn(\"row_num\", func.row_number().over(window_spec))\n",
    "\n",
    "    # create label for grouping \n",
    "    df = df.withColumn(\"group_id\", ((func.col(\"row_num\") - 1) / num_s).cast(\"int\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab5cd4c-e10a-4703-b471-dc71a3285d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs for gen_features \n",
    "# I started trying to keep them short and single purpose... but I called find_peaks 5 times\n",
    "# for the fft features there is one large UDF for 4 features\n",
    "\n",
    "# Element-wise summation of x, y, z streams\n",
    "@pandas_udf(ArrayType(DoubleType())) #schema returned \n",
    "def sum_xyz_udf(x_col: pd.Series, y_col: pd.Series, z_col: pd.Series) -> pd.Series: #(param: type) -> return type \n",
    "    return pd.Series([(np.array(x) + np.array(y) + np.array(z)).tolist() for x, y, z in zip(x_col, y_col, z_col)])\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def num_peaks_udf(col: pd.Series) -> pd.Series:\n",
    "    return col.apply(lambda x: len(find_peaks(np.array(x), distance=10)[0]))\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def num_peaks_udf_t_1(col: pd.Series) -> pd.Series:\n",
    "    return col.apply(lambda x: len(find_peaks(np.array(x), threshold=1)[0]))\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def num_peaks_udf_t_pt1(col: pd.Series) -> pd.Series:\n",
    "    return col.apply(lambda x: len(find_peaks(np.array(x), threshold=0.1)[0]))\n",
    "\n",
    "@pandas_udf(ArrayType(IntegerType()))\n",
    "def peak_locs_udf(col: pd.Series) -> pd.Series:\n",
    "    return col.apply(lambda x: find_peaks(np.array(x), distance=10)[0].tolist())\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def sum_peak_locs_udf(col: pd.Series) -> pd.Series:\n",
    "    return col.apply(lambda x: sum(find_peaks(np.array(x))[0]))\n",
    "\n",
    "\n",
    "@pandas_udf(StructType([\n",
    "        StructField(\"dom_freq\", DoubleType()),\n",
    "        StructField(\"second_freq\", DoubleType()),\n",
    "        StructField(\"third_freq\", DoubleType()),\n",
    "        StructField(\"mean_freq\", DoubleType())\n",
    "    ]))\n",
    "def fft_features_udf(col: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    calls a extract feature function that provides top 3 frequencies and the mean freq from a fft\n",
    "    if fft doesn't find enough freq it returns 0 \n",
    "    if the values are null there are not enough values in the serries \n",
    "    \"\"\"\n",
    "    def extract_features(x):\n",
    "        \"\"\"\n",
    "        expects pd.Serries, tuple of 4 doubles: (dom_freq, second_freq, third_freq, mean_freq)\n",
    "        \"\"\"\n",
    "        SAMPLE_RATE = 20 #Hz\n",
    "\n",
    "        # if no value  \n",
    "        output = 0.0 \n",
    "        # output = None \n",
    "        \n",
    "        x = np.array(x)\n",
    "        if len(x) < 2:\n",
    "            return (None, None, None, None)\n",
    "        \n",
    "        x = x - np.mean(x) #remove offset\n",
    "\n",
    "        # FFT\n",
    "        fft_vals = np.fft.fft(x)\n",
    "        fft_freq = np.fft.fftfreq(len(x), d=1.0/SAMPLE_RATE)\n",
    "\n",
    "        # Use only the positive spectrum\n",
    "        pos = fft_freq > 0\n",
    "        freqs = fft_freq[pos]\n",
    "        mag_s = np.abs(fft_vals[pos])  # Magnitude spectrum\n",
    "\n",
    "        if len(mag_s) >= 3: \n",
    "            top_freqs = freqs[np.argsort(mag_s)[-3:]]\n",
    "            \n",
    "            dom_freq = top_freqs[-1]\n",
    "            second_freq = top_freqs[-2]\n",
    "            third_freq = top_freqs[-3]\n",
    "            \n",
    "        elif len(mag_s) == 2:\n",
    "            top_freqs = freqs[np.argsort(mag_s)[-2:]]\n",
    "\n",
    "            dom_freq = top_freqs[-1]\n",
    "            second_freq = top_freqs[-2]\n",
    "            third_freq = output\n",
    "            \n",
    "        elif len(mag_s) == 1: \n",
    "            top_freqs = freqs[np.argsort(mag_s)[-1:]]\n",
    "\n",
    "            dom_freq = top_freqs[-1]\n",
    "            second_freq = output\n",
    "            third_freq = output\n",
    "        else: \n",
    "            dom_freq = output\n",
    "            second_freq = output\n",
    "            third_freq = output\n",
    "\n",
    "        # Dominant frequency\n",
    "        # top_freqs = freqs[np.argsort(mag_s)[-3:]] if len(mag_s) > 3 else [None, None, None]\n",
    "        # dom_freq = top_freqs[-1]\n",
    "        # second_freq = top_freqs[-2]\n",
    "        # third_freq = top_freqs[-3]\n",
    "        # # dom_freq = freqs[np.argmax(mags)] if len(mags) > 0 else None\n",
    "        \n",
    "        # Mean freq (weighted)\n",
    "        mean_freq = np.sum(freqs * mag_s) / np.sum(mag_s) if np.sum(mag_s) > 0 else output\n",
    "\n",
    "        return (dom_freq, second_freq, third_freq, mean_freq)\n",
    "    \n",
    "    return pd.DataFrame(col.apply(extract_features).tolist(), columns=[\"dom_freq\", \"second_freq\", \"third_freq\", \"mean_freq\"])\n",
    "\n",
    "\n",
    "\n",
    "def gen_features(df): \n",
    "    \"\"\" takes a spark df\n",
    "\n",
    "    dfs must have columns \"sub_id\", \"activity\", \"group_id\"\n",
    "    \"\"\"\n",
    "    df = df.groupBy(\"sub_id\", \"activity\", \"group_id\").agg(\n",
    "        func.collect_list(\"time\").alias(\"time\"), \n",
    "        func.collect_list(\"x\").alias(\"x\"),\n",
    "        func.collect_list(\"y\").alias(\"y\"),\n",
    "        func.collect_list(\"z\").alias(\"z\"), \n",
    "        func.avg(\"x\").alias(\"x_avg\"),\n",
    "        func.avg(\"y\").alias(\"y_avg\"), \n",
    "        func.avg(\"z\").alias(\"z_avg\"),\n",
    "        func.sum(\"x\").alias(\"x_sum\"),\n",
    "        func.sum(\"y\").alias(\"y_sum\"),\n",
    "        func.sum(\"z\").alias(\"z_sum\"),\n",
    "        func.stddev(\"x\").alias(\"x_stddev\"),\n",
    "        func.stddev(\"y\").alias(\"y_stddev\"),\n",
    "        func.stddev(\"z\").alias(\"z_stddev\"),\n",
    "        func.corr(\"x\", \"y\").alias(\"corr_x_y\"),\n",
    "        func.corr(\"x\", \"z\").alias(\"corr_x_z\"),\n",
    "        func.corr(\"y\", \"z\").alias(\"corr_y_z\"),\n",
    "        func.avg( func.sqrt( func.pow(\"x\", 2) + func.pow(\"y\", 2) + func.pow(\"z\", 2))).alias(\"resultant\")\n",
    "        \n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"sum_xyz\", sum_xyz_udf(\"x\", \"y\", \"z\"))\n",
    "    \n",
    "    # find_peaks \n",
    "    df = df.withColumn(\"x_num_peaks\", num_peaks_udf(\"x\")) \\\n",
    "    .withColumn(\"x_num_peaks_t_1\", num_peaks_udf_t_1('x')) \\\n",
    "    .withColumn(\"x_num_peaks_t_pt1\", num_peaks_udf_t_pt1('x'))\n",
    "\n",
    "    df = df.withColumn(\"y_num_peaks\", num_peaks_udf(\"y\")) \\\n",
    "    .withColumn(\"y_num_peaks_t_1\", num_peaks_udf_t_1('y')) \\\n",
    "    .withColumn(\"y_num_peaks_t_pt1\", num_peaks_udf_t_pt1('y'))\n",
    "\n",
    "    df = df.withColumn(\"z_num_peaks\", num_peaks_udf(\"z\")) \\\n",
    "    .withColumn(\"z_num_peaks_t_1\", num_peaks_udf_t_1('z')) \\\n",
    "    .withColumn(\"z_num_peaks_t_pt1\", num_peaks_udf_t_pt1('z'))\n",
    "\n",
    "    df = df.withColumn(\"xyz_num_peaks\", num_peaks_udf(\"sum_xyz\")) \\\n",
    "    .withColumn(\"xyz_num_peaks_t_1\", num_peaks_udf_t_1('sum_xyz')) \\\n",
    "    .withColumn(\"xyz_num_peaks_t_pt1\", num_peaks_udf_t_pt1('sum_xyz'))\n",
    "\n",
    "    # might have some information on where in the segment the peaks are found relative\n",
    "    df = df.withColumn(\"x_avg_peak_locs\", sum_peak_locs_udf(\"x\")/col(\"x_num_peaks\")) \\\n",
    "    .withColumn(\"y_avg_peak_locs\", sum_peak_locs_udf(\"y\")/col(\"y_num_peaks\")) \\\n",
    "    .withColumn(\"z_avg_peak_locs\", sum_peak_locs_udf(\"z\")/col(\"z_num_peaks\")) \\\n",
    "    .withColumn(\"xyz_avg_peak_locs\", sum_peak_locs_udf(\"sum_xyz\")/col(\"xyz_num_peaks\")) \\\n",
    "    \n",
    "    \n",
    "    # frequency features \n",
    "    # each block unpacks the udf results into 4 features \n",
    "    df = df.withColumn(\"x_fft\", fft_features_udf(\"x\")) \\\n",
    "        .withColumn(\"x_dom_freq\", func.col(\"x_fft.dom_freq\")) \\\n",
    "        .withColumn(\"x_2nd_freq\", func.col(\"x_fft.second_freq\")) \\\n",
    "        .withColumn(\"x_3rd_freq\", func.col(\"x_fft.third_freq\")) \\\n",
    "        .withColumn(\"x_mean_freq\", func.col(\"x_fft.mean_freq\")) \\\n",
    "        .drop(\"x_fft\")\n",
    "\n",
    "    df = df.withColumn(\"y_fft\", fft_features_udf(\"y\")) \\\n",
    "        .withColumn(\"y_dom_freq\", func.col(\"y_fft.dom_freq\")) \\\n",
    "        .withColumn(\"y_2nd_freq\", func.col(\"y_fft.second_freq\")) \\\n",
    "        .withColumn(\"y_3rd_freq\", func.col(\"y_fft.third_freq\")) \\\n",
    "        .withColumn(\"y_mean_freq\", func.col(\"y_fft.mean_freq\")) \\\n",
    "        .drop(\"y_fft\")\n",
    "\n",
    "    df = df.withColumn(\"z_fft\", fft_features_udf(\"z\")) \\\n",
    "        .withColumn(\"z_dom_freq\", func.col(\"z_fft.dom_freq\")) \\\n",
    "        .withColumn(\"z_2nd_freq\", func.col(\"z_fft.second_freq\")) \\\n",
    "        .withColumn(\"z_3rd_freq\", func.col(\"z_fft.third_freq\")) \\\n",
    "        .withColumn(\"z_mean_freq\", func.col(\"z_fft.mean_freq\")) \\\n",
    "        .drop(\"z_fft\")\n",
    "    \n",
    "    df = df.withColumn(\"xyz_fft\", fft_features_udf(\"sum_xyz\")) \\\n",
    "        .withColumn(\"xyz_dom_freq\", func.col(\"xyz_fft.dom_freq\")) \\\n",
    "        .withColumn(\"xyz_2nd_freq\", func.col(\"xyz_fft.second_freq\")) \\\n",
    "        .withColumn(\"xyz_3rd_freq\", func.col(\"xyz_fft.third_freq\")) \\\n",
    "        .withColumn(\"xyz_mean_freq\", func.col(\"xyz_fft.mean_freq\")) \\\n",
    "        .drop(\"xyz_fft\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_phone_a_seg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033e29b-27de-4bc2-af77-830ae33a6710",
   "metadata": {},
   "source": [
    "### Apply Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "badb7865-e8fc-4f0d-9e87-5e59209ce720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows_dropped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows_dropped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows_dropped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows_dropped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_phone_a = clean_df(df_phone_a)\n",
    "df_phone_g = clean_df(df_phone_g)\n",
    "df_watch_a = clean_df(df_watch_a)\n",
    "df_watch_g = clean_df(df_watch_g)\n",
    "\n",
    "# df_phone_a_seg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3a624-bc75-4b01-b29f-259d628c7f39",
   "metadata": {},
   "source": [
    "### Apply Feature Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee03b33-8ce2-4918-ac99-46dff8208788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here SAMPLE_RATE = 20Hz and SECONDS = 10 --> 200 sample segments \n",
    "df_phone_a_seg = gen_features(label_segments(df_phone_a, SAMPLE_RATE*SECONDS))\n",
    "df_phone_g_seg = gen_features(label_segments(df_phone_g, SAMPLE_RATE*SECONDS))\n",
    "df_watch_a_seg = gen_features(label_segments(df_watch_a, SAMPLE_RATE*SECONDS))\n",
    "df_watch_g_seg = gen_features(label_segments(df_watch_g, SAMPLE_RATE*SECONDS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adfe10c-e468-4899-8d3e-18269c995873",
   "metadata": {},
   "source": [
    "### Clean Up Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5382e7ba-cad4-4cf8-be6a-90f0892fd5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that have too few samples \n",
    "min_samples = SAMPLE_RATE*SECONDS\n",
    "\n",
    "df_phone_a_seg = df_phone_a_seg.filter(func.size(col(\"x\")) == min_samples)\n",
    "df_phone_g_seg = df_phone_g_seg.filter(func.size(col(\"x\")) == min_samples)\n",
    "df_watch_a_seg = df_watch_a_seg.filter(func.size(col(\"x\")) == min_samples)\n",
    "df_watch_g_seg = df_watch_g_seg.filter(func.size(col(\"x\")) == min_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c46281-ce2c-450a-9d5d-000340d28d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Select a df with sub_id, activity, and generated features (numeric cols)\n",
    "numeric_cols = [field.name for field in df_watch_a_seg.schema.fields if isinstance(field.dataType, IntegerType) or isinstance(field.dataType, DoubleType)]\n",
    "to_select = [\"activity\"] + numeric_cols \n",
    "\n",
    "\n",
    "pddf_phone_a = df_phone_a_seg.select(to_select).toPandas()\n",
    "pddf_watch_a = df_watch_a_seg.select(to_select).toPandas()\n",
    "pddf_phone_g = df_phone_g_seg.select(to_select).toPandas()\n",
    "pddf_watch_g = df_watch_g_seg.select(to_select).toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0468b9-c338-4ab9-a9db-287730e87eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save CSV for future work \n",
    "path = \"/home/work/Final/features_\"\n",
    "\n",
    "pddf_phone_a.to_csv(path+\"phone_a\"+\".csv\", index=False)\n",
    "pddf_watch_a.to_csv(path+\"watch_a\"+\".csv\", index=False) \n",
    "pddf_phone_g.to_csv(path+\"phone_g\"+\".csv\", index=False) \n",
    "pddf_watch_g.to_csv(path+\"watch_g\"+\".csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f80921-7174-4efa-93f9-059935d606f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
